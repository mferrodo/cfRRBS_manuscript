{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('Agg')\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import sys, getopt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parallelisation options\n",
    "import multiprocessing\n",
    "from multiprocessing import Process, Manager, Pool\n",
    "cpuCount = (multiprocessing.cpu_count() - 2)\n",
    "cpuCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEDtools > 2.27.1 is needed!\n",
    "### Tested with py >3.4\n",
    "## More information is at https://github.com/shulik7/CancerLocator/tree/Add_instruction_and_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edit\n",
    "\n",
    "## Read arguments from command line\n",
    "#parser = argparse.ArgumentParser('MakeTrainTest.py')\n",
    "#parser.add_argument('-t', '--train', default = 'disabled')\n",
    "#parser.add_argument('-v', '--viz', default = 'disabled')\n",
    "#args = parser.parse_args()\n",
    "#train = args.train\n",
    "#visualisation = args.viz\n",
    "\n",
    "train='disabled'\n",
    "visualisation='disabled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the training set is: disabled\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating the training set is:\", train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lored/Documents/Design_Project/cfRRBS_manuscript/code'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Edit\n",
    "\n",
    "os.chdir(\"./\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of output\n",
    "testMethyName = \"test_methy\"\n",
    "testDepthName = \"test_depth\"\n",
    "trainFileName = \"train\"\n",
    "testBetaName = \"test_beta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder to store intermediate files\n",
    "tmp_folder = \"./classifySamples/processed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./classifySamples/testfiles/DNA019089_small_S4_R1_001dedupl.cov',\n",
       " './classifySamples/testfiles/DNA019089_small_S4_R1_001.cov']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Edit\n",
    "\n",
    "## The location of the cfRRBS files, after running the preprocessing pipeline\n",
    "test_folder = \"./classifySamples/testfiles/\"\n",
    "test_files = glob.glob(os.path.join(test_folder, \"*.cov\"))\n",
    "test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infinium data for the reference dataset\n",
    "NBL_infinium_folder = \"./classifySamples/train/NBL\"\n",
    "NBL_infinium_files = glob.glob(os.path.join(NBL_infinium_folder, \"*.txt\"))\n",
    "OS_infinium_folder = \"./classifySamples/train/OS\"\n",
    "OS_infinium_files = glob.glob(os.path.join(OS_infinium_folder, \"*.txt\"))\n",
    "CCSK_infinium_folder = \"./classifySamples/train/CCSK\"\n",
    "CCSK_infinium_files = glob.glob(os.path.join(CCSK_infinium_folder, \"*.txt\"))\n",
    "WBC_infinium_folder = \"./classifySamples/train/WBC_child/\"\n",
    "WBC_infinium_files = glob.glob(os.path.join(WBC_infinium_folder, \"*.txt\"))\n",
    "WT_infinium_folder = \"./classifySamples/train/WT/\"\n",
    "WT_infinium_files = glob.glob(os.path.join(WT_infinium_folder, \"*.txt\"))\n",
    "EWS_infinium_folder = \"./classifySamples/train/EWS/450k\"\n",
    "EWS_infinium_files = glob.glob(os.path.join(EWS_infinium_folder, \"*.txt\"))\n",
    "RMS_infinium_folder = \"./classifySamples/train/RMS\"\n",
    "RMS_infinium_files = glob.glob(os.path.join(RMS_infinium_folder, \"*.txt\"))\n",
    "MRT_infinium_folder = \"./classifySamples/train/MRT\"\n",
    "MRT_infinium_files = glob.glob(os.path.join(MRT_infinium_folder, \"*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGBS data for the reference dataset.\n",
    "NRML_WGBS_folder = \"./classifySamples/train/NRML/\"\n",
    "NRML_WGBS_files = glob.glob(os.path.join(NRML_WGBS_folder, \"*.cov\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper script, that if you have a m x n matrix (columns being samples and lines being CpGs), that splits the matrix into m x 1 and saves it back again. It is useful for preprocessing, but is not used here in the script.\n",
    "# def splitMatrix(inputfile, outputfolder):\n",
    "#     matrix = pd.read_csv(inputfile, sep=\",\", header=0, index_col=0)\n",
    "#     for i in range(len(matrix.columns)):\n",
    "#         name = list(matrix.columns.values)\n",
    "#         name = name[i]\n",
    "#         print(\"Writing %s.txt\" % name)\n",
    "#         matrix.to_csv(outputfolder + \"%s.txt\" % name , header=0, index=True,columns=[name], sep='\\t', mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0         1         2      3\n",
      "0       1     29165     29483      0\n",
      "1       1    567122    567401      1\n",
      "2       1    762982    763181      2\n",
      "3       1    844314    845703      3\n",
      "4       1    859816    860118      4\n",
      "...    ..       ...       ...    ...\n",
      "14098  22  51066374  51066551  14098\n",
      "14099  22  51066655  51066815  14099\n",
      "14100  22  51066832  51066936  14100\n",
      "14101  22  51142514  51143306  14101\n",
      "14102  22  51221840  51222150  14102\n",
      "\n",
      "[14103 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# The file containing the features (= the intersect between HM450K data and RRBS data, see GitHub README)\n",
    "\n",
    "#Edit important: resource folder didn't exist yet. I created the folder and copied the file. Maybe it's better to refer to the original file?\n",
    "\n",
    "clusters = pd.read_csv(\"./classifySamples/resources/RRBS_450k_intersectClusters.tsv\", sep=\"\\t\",usecols=[0,1,2], skiprows=[0], header=None, index_col=None)\n",
    "clusters[3] = clusters.index\n",
    "\n",
    "#Edit\n",
    "\n",
    "print(clusters)\n",
    "\n",
    "clusterFile = \"RRBS_450k_intersectClusters\"\n",
    "\n",
    "#Edit important: processed folder did not exist yet. Must be initialized first\n",
    "\n",
    "clusters.to_csv(tmp_folder + \"%s.txt\" % clusterFile, header=None, index=None, sep='\\t', mode = 'w')\n",
    "clusters = clusters.drop([0,1,2,3], axis = 1) # Use empty index to later extract all the clusters from, so that every sample has the same number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHR</th>\n",
       "      <th>MAPINFO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>cg00035864</td>\n",
       "      <td>Y</td>\n",
       "      <td>8553009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cg00050873</td>\n",
       "      <td>Y</td>\n",
       "      <td>9363356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cg00061679</td>\n",
       "      <td>Y</td>\n",
       "      <td>25314171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cg00063477</td>\n",
       "      <td>Y</td>\n",
       "      <td>22741795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cg00121626</td>\n",
       "      <td>Y</td>\n",
       "      <td>21664296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cg27657537</td>\n",
       "      <td>22</td>\n",
       "      <td>20863762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cg27660038</td>\n",
       "      <td>22</td>\n",
       "      <td>20378532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cg27662284</td>\n",
       "      <td>22</td>\n",
       "      <td>20342520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cg27662611</td>\n",
       "      <td>22</td>\n",
       "      <td>38598981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cg27665648</td>\n",
       "      <td>22</td>\n",
       "      <td>30112403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>482421 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           CHR   MAPINFO\n",
       "cg00035864   Y   8553009\n",
       "cg00050873   Y   9363356\n",
       "cg00061679   Y  25314171\n",
       "cg00063477   Y  22741795\n",
       "cg00121626   Y  21664296\n",
       "...         ..       ...\n",
       "cg27657537  22  20863762\n",
       "cg27660038  22  20378532\n",
       "cg27662284  22  20342520\n",
       "cg27662611  22  38598981\n",
       "cg27665648  22  30112403\n",
       "\n",
       "[482421 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load HumanMethylation450K reference file\n",
    "\n",
    "#Edit important: I downloaded this csv file myself.\n",
    "\n",
    "array450k = pd.read_csv(\"./classifySamples/resources/HumanMethylation450_15017482_v1-2.csv\", dtype={\"CHR\": str}, header = 7, usecols = (0,10,11,12), index_col=\"IlmnID\")\n",
    "array450k = array450k.dropna()\n",
    "array450k[['MAPINFO', 'Genome_Build']] = array450k[['MAPINFO', 'Genome_Build']].astype(int)\n",
    "array450k = array450k[array450k['Genome_Build'] == 37] # Extract locations with genome build GRCh37\n",
    "array450k = array450k.drop(['Genome_Build'], axis = 1)\n",
    "array450k[['CHR', 'MAPINFO']] = array450k[['CHR', 'MAPINFO']].astype(str)\n",
    "array450k.index.name = None\n",
    "\n",
    "#Edit\n",
    "\n",
    "array450k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHR</th>\n",
       "      <th>MAPINFO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>cg07881041</td>\n",
       "      <td>19</td>\n",
       "      <td>5236016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cg18478105</td>\n",
       "      <td>20</td>\n",
       "      <td>61847650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cg23229610</td>\n",
       "      <td>1</td>\n",
       "      <td>6841125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cg03513874</td>\n",
       "      <td>2</td>\n",
       "      <td>198303466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cg09835024</td>\n",
       "      <td>X</td>\n",
       "      <td>24072640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cg23079522</td>\n",
       "      <td>3</td>\n",
       "      <td>160569628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cg16818145</td>\n",
       "      <td>3</td>\n",
       "      <td>182782277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cg14585103</td>\n",
       "      <td>8</td>\n",
       "      <td>139940608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cg10633746</td>\n",
       "      <td>17</td>\n",
       "      <td>18164442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cg12623625</td>\n",
       "      <td>1</td>\n",
       "      <td>17946923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>862927 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           CHR    MAPINFO\n",
       "cg07881041  19    5236016\n",
       "cg18478105  20   61847650\n",
       "cg23229610   1    6841125\n",
       "cg03513874   2  198303466\n",
       "cg09835024   X   24072640\n",
       "...         ..        ...\n",
       "cg23079522   3  160569628\n",
       "cg16818145   3  182782277\n",
       "cg14585103   8  139940608\n",
       "cg10633746  17   18164442\n",
       "cg12623625   1   17946923\n",
       "\n",
       "[862927 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MethylationEPIC reference file\n",
    "\n",
    "#Edit important: I downloaded this csv file myself.\n",
    "\n",
    "array850k = pd.read_csv(\"./classifySamples/resources/MethylationEPIC_v-1-0_B4.csv\", dtype={\"CHR\": str}, header = 7, usecols = (0,10,11,12), index_col=\"IlmnID\")\n",
    "array850k = array850k.dropna()\n",
    "array850k[['MAPINFO', 'Genome_Build']] = array850k[['MAPINFO', 'Genome_Build']].astype(int)\n",
    "array850k = array850k[array850k['Genome_Build'] == 37] # Extract locations with genome build GRCh37\n",
    "array850k = array850k.drop(['Genome_Build'], axis = 1)\n",
    "array850k[['CHR', 'MAPINFO']] = array850k[['CHR', 'MAPINFO']].astype(str)\n",
    "array850k.index.name = None\n",
    "\n",
    "#Edit\n",
    "\n",
    "array850k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating test_depth and test_methy\n"
     ]
    }
   ],
   "source": [
    "# Process test files for input in ruMeth_atlas.py\n",
    "print(\"Generating %s and %s\" % (testDepthName, testMethyName))\n",
    "def import_test_files(x):\n",
    "        # Goal: to obtain one file, containing all the test files, where the first column is all the samples and the rest of the column either the beta values, total depth or # methylated reads for that cluster.\n",
    "        # 1. Read in the bismark coverage file and convert them to a sort-of bed file, so that it can be manipulated with bedtools.\n",
    "        file = x\n",
    "        file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "        \n",
    "        #Edit important\n",
    "        \n",
    "        df = pd.read_csv(file, sep=\"\\t\",usecols=[0,1,2,3,4,5], header=None, engine='python')\n",
    "        df[3] = df[3]/100    # From methylation percentage to methylation ratio\n",
    "        df.to_csv(tmp_folder + \"%s.txt\" % file_name , header=None, index=None, sep='\\t', mode = 'w')\n",
    "\n",
    "        # 2. Intersect between the cluster file and the bismark coverage file\n",
    "        outfile = open(tmp_folder + '%s_intersect.txt' % file_name, 'w')\n",
    "        print(\"     Running bedtools intersect on %s.txt...\" % file_name)\n",
    "        arg = \"bedtools intersect -wb -b %s/%s.txt -a %s%s.txt\" % (tmp_folder, clusterFile, tmp_folder, file_name)\n",
    "        arg = arg.split()\n",
    "        proc = subprocess.Popen(args=arg, stdout=outfile, universal_newlines=True).wait()\n",
    "        df = pd.read_csv(tmp_folder + '%s_intersect.txt' % file_name, sep=\"\\t\", usecols=[6,7,8,3,4,5,9], header=None) # The previous step shuffles the column order, so this step rearranges the column order\n",
    "        df = df[[6,7,8,3,4,5,9]] # chr, start, stop, beta value, count methylated, count unmethylated, cluster number\n",
    "        df.to_csv(tmp_folder + \"%s_reordered.txt\" % file_name , header=None, index=None, sep='\\t', mode = 'w')\n",
    "\n",
    "        # 3. Group all the rows that are within a cluster, and get the sum of the methylated and unmethylated values. In addition, get the row with the CpG cluster number for future indexing.\n",
    "        arg = \"bedtools groupby -i %s%s_reordered.txt -g 1-3,7 -c 5,6 -o sum\" % (tmp_folder, file_name)\n",
    "        arg = arg.split()\n",
    "        outfile = open(tmp_folder + '%s_clustered.txt' % file_name, 'w')\n",
    "        print(\"     Running bedtools groupby on %s.txt...\" %file_name)\n",
    "        proc = subprocess.Popen(args=arg, stdout=outfile, universal_newlines=True).wait()\n",
    "\n",
    "        # 4. Remove all clusters that have less than 30 reads\n",
    "        df = pd.read_csv(tmp_folder + '%s_clustered.txt' % file_name, sep=\"\\t\", header=None, index_col = 3 )\n",
    "        df.index.name = None    # Remove index.name for consistency\n",
    "        df[6] = df[4]/(df[4] + df[5])   # Get beta value per cluster\n",
    "        df = df[[0,1,2,6,4,5]] # Reorder the columns in chr, start, stop, beta value, no methylated, no unmethylated\n",
    "        df.sort_values(by=[0,1,2], inplace=True) # Sort by chromosome\n",
    "        df[7] = df[4] + df[5]   # Get total depth (=methylated + unmethylated count)\n",
    "        df[[7,6,4,5]] = df[[7,6,4,5]].mask(df[7] < 30)  # Mark all clusters lower than 30 reads with NA\n",
    "        print(\"The amount of clusters in %s remaining after removing NA: %s\" % (file_name, len(df[7].replace(np.nan, 'NA').apply(pd.to_numeric, errors='coerce').dropna())))\n",
    "        df = df.replace(np.nan, 'NA', regex=True) # Replace numpy NaN with NA string\n",
    "        print(\"     Extracting %s and %s from file %s.txt...\" % (testMethyName, testDepthName, file_name))\n",
    "\n",
    "        # 5. Add the first file to the testMethy_list ( = number of methylated CpGs per cluster)\n",
    "        testMethy_df = df\n",
    "        testMethy_df.columns = [0,1,2,6, \"%s\" % file_name, 5,7]\n",
    "        testMethy_df = testMethy_df.drop([0,1,2,6,5,7], axis = 1).astype(str)\n",
    "        testMethy_df[file_name] = testMethy_df[file_name].apply(lambda x: x.split('.')[0]) # Make integer of float numbers, but as the dataframe is astype(str), we need to do this with a lambda function\n",
    "        print(testMethy_list)\n",
    "        print(\"test\")\n",
    "        print(testMethy_df)\n",
    "        testMethy_list.append(testMethy_df)\n",
    "\n",
    "        # Identical to testMethy\n",
    "        testDepth_df = df\n",
    "        testDepth_df.columns = [0,1,2,5,3,4,\"%s\" % file_name]\n",
    "        testDepth_df = testDepth_df.drop([0,1,2,3,4,5], axis = 1).astype(str)\n",
    "        testDepth_df[file_name] = testDepth_df[file_name].apply(lambda x: x.split('.')[0])\n",
    "        testDepth_list.append(testDepth_df)\n",
    "\n",
    "        # Make a new variable for visualisation that contains the beta values of the clusters\n",
    "        testBeta_df = df\n",
    "        testBeta_df.columns = [0,1,2, \"%s\" % file_name, 4, 5,7]\n",
    "        testBeta_df = testBeta_df.drop([0,1,2,4,5,7], axis = 1).astype(str)\n",
    "        testBeta_list.append(testBeta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Running bedtools intersect on DNA019089_small_S4_R1_001.txt...\n",
      "     Running bedtools intersect on DNA019089_small_S4_R1_001dedupl.txt...\n",
      "     Running bedtools groupby on DNA019089_small_S4_R1_001.txt...\n",
      "     Running bedtools groupby on DNA019089_small_S4_R1_001dedupl.txt...\n",
      "The amount of clusters in DNA019089_small_S4_R1_001 remaining after removing NA: 0\n",
      "     Extracting test_methy and test_depth from file DNA019089_small_S4_R1_001.txt...\n",
      "[]\n",
      "test\n",
      "      DNA019089_small_S4_R1_001\n",
      "20                           NA\n",
      "38                           NA\n",
      "39                           NA\n",
      "93                           NA\n",
      "100                          NA\n",
      "...                         ...\n",
      "14043                        NA\n",
      "14051                        NA\n",
      "14060                        NA\n",
      "14071                        NA\n",
      "14102                        NA\n",
      "\n",
      "[1144 rows x 1 columns]The amount of clusters in DNA019089_small_S4_R1_001dedupl remaining after removing NA: 0\n",
      "\n",
      "     Extracting test_methy and test_depth from file DNA019089_small_S4_R1_001dedupl.txt...\n",
      "[      DNA019089_small_S4_R1_001\n",
      "20                           NA\n",
      "38                           NA\n",
      "39                           NA\n",
      "93                           NA\n",
      "100                          NA\n",
      "...                         ...\n",
      "14043                        NA\n",
      "14051                        NA\n",
      "14060                        NA\n",
      "14071                        NA\n",
      "14102                        NA\n",
      "\n",
      "[1144 rows x 1 columns]]\n",
      "test\n",
      "      DNA019089_small_S4_R1_001dedupl\n",
      "20                                 NA\n",
      "38                                 NA\n",
      "39                                 NA\n",
      "93                                 NA\n",
      "100                                NA\n",
      "...                               ...\n",
      "14043                              NA\n",
      "14051                              NA\n",
      "14060                              NA\n",
      "14071                              NA\n",
      "14102                              NA\n",
      "\n",
      "[1144 rows x 1 columns]\n",
      "Merging all test_methy in one file...\n",
      "Merging all test_depth in one file...\n",
      "Writing to disk...\n",
      "The number of columns in the test_methy file after removing NA values is: 0\n",
      "The number of columns in the test_depth file after removing NA values is: 0\n"
     ]
    }
   ],
   "source": [
    "# Use the manager package so the lists are shared between the processes\n",
    "with Manager() as manager:\n",
    "    # Define empty lists\n",
    "    testMethy_list = manager.list()\n",
    "    testDepth_list = manager.list()\n",
    "    testBeta_list  = manager.list()\n",
    "\n",
    "    pool = Pool(cpuCount)  # Parallelisation function\n",
    "\n",
    "    pool.map(import_test_files, test_files)    # Import the files in parallel\n",
    "\n",
    "    print(\"Merging all %s in one file...\" % testMethyName)  # Merge the testMethy_list in a pandas dataframe, merging the same indices\n",
    "    testMethy = pd.concat(testMethy_list, axis = 1)\n",
    "    testMethy = pd.merge(clusters, testMethy, how = \"left\", left_index=True, right_index=True)    # Merge the pandas df with the clusters, leaving NA values for clusters that were not covered.\n",
    "    testMethy = testMethy.transpose().fillna('NA')   # Transpose and Fill NaN with NA string\n",
    "    testMethy.to_csv(\"./classifySamples/output/%s\" % testMethyName, header=None,sep='\\t', mode = 'w')\n",
    "\n",
    "    print(\"Merging all test_depth in one file...\")\n",
    "    testDepth = pd.concat(testDepth_list, axis = 1)\n",
    "    testDepth = pd.merge(clusters, testDepth, how = \"left\", left_index=True, right_index=True)\n",
    "    testDepth = testDepth.transpose().fillna('NA')\n",
    "    testDepth.to_csv(\"./classifySamples/output/%s\" % testDepthName, header=None,sep='\\t', mode = 'w')\n",
    "\n",
    "    testBeta = pd.concat(testBeta_list, axis = 1)\n",
    "    testBeta = pd.merge(clusters, testBeta, how = \"left\", left_index=True, right_index=True)\n",
    "    testBeta = testBeta.transpose().fillna('NA')\n",
    "    print(\"Writing to disk...\")\n",
    "    testBeta.to_csv(\"./classifySamples/output/%s\" % testBetaName, header=None,sep='\\t', mode = 'w')\n",
    "\n",
    "    testMethy_rmNA = testMethy.apply(pd.to_numeric, errors='coerce').dropna(axis=1)\n",
    "    testDepth_rmNA = testDepth.apply(pd.to_numeric, errors='coerce').dropna(axis=1)\n",
    "    print(\"The number of columns in the %s file after removing NA values is: %i\" % (testMethyName,testMethy_rmNA.shape[1]))\n",
    "    print(\"The number of columns in the %s file after removing NA values is: %i\" % (testDepthName,testDepth_rmNA.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the reference set, only needs to be done once, or every time new files are added to the reference dataset. Because the reference set is derived from multiple sources of publicly available data, every reference entity has it's own import (and no universal function for was written, because that would need additional preprocessing steps).\n",
    "if train == 'enabled':\n",
    "    print(\"Generating %s\" % trainFileName)\n",
    "    def getAvg(x):\n",
    "        ## This function gets the average beta value in a cluster, or writes NA if more than half are not available.\n",
    "        if isinstance(x, float):\n",
    "            return x\n",
    "        elif len(x) == 0:\n",
    "            x = \"NA\"\n",
    "        else:\n",
    "            line_values = []\n",
    "            countNA = 0\n",
    "            line_values = x.split(',')\n",
    "            line_values = [i.strip(' ') for i in line_values]\n",
    "            line_values = list(filter(None, line_values))\n",
    "            countTot = len(line_values)\n",
    "            for value in line_values:\n",
    "                if value == 'NA':\n",
    "                    countNA = countNA +1\n",
    "            if countTot == 0:\n",
    "                x = \"NA\"\n",
    "            elif countNA/countTot >= 0.5:\n",
    "                x = \"NA\"\n",
    "            else:\n",
    "                line_values_rmNA = filter(lambda a: a != 'NA', line_values)\n",
    "                line_values_rmNA_list = list(line_values_rmNA)\n",
    "                calcmean = np.array(line_values_rmNA_list).astype(np.float)\n",
    "                x = np.mean(calcmean)\n",
    "            return x\n",
    "\n",
    "\n",
    "    def generateTrain_Infinium(label, file_name):\n",
    "        # The input for this function is an ordered infinium 450k file with the order chr - start - stop - beta value. If it doesn't have this structure, some preprocessing needs to be done.\n",
    "        outfile = open(tmp_folder + \"%s_intersect.txt\" % file_name, 'w')\n",
    "        print(\"     Running bedtools intersect on %s.txt...\" % file_name)\n",
    "        proc = subprocess.Popen(args=[\"bedtools\", \"intersect\", \"-b\", tmp_folder + \"%s.txt\" % clusterFile, \"-a\", tmp_folder + \"%s.txt\" % file_name, \"-wb\"], stdout=outfile, universal_newlines=True).wait()\n",
    "\n",
    "        df = pd.read_csv(tmp_folder + '%s_intersect.txt' % file_name, sep=\"\\t\", usecols=[6,3,4,5,7], header=None )\n",
    "        df = df[[4,5,6,3,7]]\n",
    "        df[3] = df[3].replace(np.nan, 'NA', regex=True)\n",
    "        df.to_csv(tmp_folder + \"%s_reordered.txt\" % file_name , header=None, index=None, sep='\\t', mode = 'w')\n",
    "\n",
    "        # Group the total and methylated reads per cluster\n",
    "        arg = \"bedtools groupby -i %s%s_reordered.txt -g 1-3,5 -c 4 -o collapse\" % (tmp_folder, file_name)\n",
    "        arg = arg.split()\n",
    "        outfile = open(tmp_folder + '%s_clustered.txt' % file_name, 'w')\n",
    "        print(\"     Running bedtools groupby on %s.txt...\" %file_name)\n",
    "        proc = subprocess.Popen(args=arg, stdout=outfile, universal_newlines=True).wait()\n",
    "\n",
    "        df = pd.read_csv(tmp_folder + '%s_clustered.txt' % file_name, sep=\"\\t\", header=None, index_col=3 )\n",
    "        df[4] = df[4].astype(str)\n",
    "        df = df.groupby([df.index,0,1,2])[4].apply(','.join) # BEDtools groupby doesnt always make the index unique for some reason, this fixes this.\n",
    "        df = df.reset_index().set_index(3)\n",
    "        df = df[~df.index.duplicated(keep='first')] # This shouldn't be necessary anymore, but keep it here as a double check\n",
    "        df.index.name = None\n",
    "        df.sort_values(by=[0,1,2], inplace=True)\n",
    "        df[4] = df[4].apply(getAvg)\n",
    "        df.columns = [0,1,2,label]\n",
    "        df = df.drop([0,1,2], axis = 1)\n",
    "        return df\n",
    "\n",
    "    def generateTrain_NGS(inputfile, label, file_name):\n",
    "        # The structure of this function is very similar to import_test_files()\n",
    "        df = pd.read_csv(inputfile, sep=\"\\t\",usecols=[0,1,2,3,4,5], header=None)\n",
    "        df[3] = df[3]/100\n",
    "        df.to_csv(tmp_folder + \"%s.txt\" % file_name , header=None, index=None, sep='\\t', mode = 'w')\n",
    "\n",
    "        outfile = open(tmp_folder + '%s_intersect.txt' % file_name, 'w')\n",
    "        print(\"     Running bedtools intersect on %s.txt...\" % file_name)\n",
    "        arg = \"bedtools intersect -wb -b %s%s.txt -a %s%s.txt\" % (tmp_folder, clusterFile, tmp_folder, file_name)\n",
    "        arg = arg.split()\n",
    "        proc = subprocess.Popen(args=arg, stdout=outfile, universal_newlines=True).wait()\n",
    "\n",
    "        df = pd.read_csv(tmp_folder + '%s_intersect.txt' % file_name, sep=\"\\t\", usecols=[6,7,8,3,4,5,9], header=None )\n",
    "        df = df[[6,7,8,3,4,5,9]]\n",
    "        df.to_csv(tmp_folder + \"%s_reordered.txt\" % file_name, header=None, index=None, sep='\\t', mode = 'w')\n",
    "\n",
    "        arg = \"bedtools groupby -i %s%s_reordered.txt -g 1-3,7 -c 5,6 -o sum\" % (tmp_folder, file_name)\n",
    "        arg = arg.split()\n",
    "        outfile = open(tmp_folder + '%s_clustered.txt' % file_name, 'w')\n",
    "        print(\"     Running bedtools groupby on %s.txt...\" %file_name)\n",
    "        proc = subprocess.Popen(args=arg, stdout=outfile, universal_newlines=True).wait()\n",
    "\n",
    "        df = pd.read_csv(tmp_folder + '%s_clustered.txt' % file_name, sep=\"\\t\", header=None, index_col = 3 )\n",
    "        df.index.name = None\n",
    "\n",
    "        df[6] = df[4]/(df[4] + df[5])  # Get beta value per cluster\n",
    "\n",
    "        df = df[[0,1,2,6,4,5]]         # Reorder\n",
    "        df.sort_values(by=[0,1,2], inplace=True)\n",
    "\n",
    "        df[7] = df[4] + df[5]         # Get total depth\n",
    "\n",
    "        df[[7,6,4,5]] = df[[7,6,4,5]].mask(df[7] < 30)         # Mark all clusters lower than 30 reads with NA\n",
    "        df = df.replace(np.nan, 'NA', regex=True)\n",
    "        df.columns = [0,1,2,label,4,5,7]\n",
    "        df = df.drop([0,1,2,4,5,7], axis = 1)\n",
    "        return df\n",
    "\n",
    "    # Similar to import_test_files\n",
    "    with Manager() as manager:\n",
    "        #Define empty list\n",
    "        trainFile_list = manager.list()\n",
    "\n",
    "        # Read in WBC files.\n",
    "        def import_WBC_train(x):\n",
    "            # 1. First, some reordering is done so that the files can be manipulated with bedtools.\n",
    "            file = x\n",
    "            file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            ## Open the file\n",
    "            df = pd.read_csv(file, sep=\"\\t\", header=None, index_col=0, names = [\"Beta_Value\"])\n",
    "            ## Add the chromosomal position to the sample\n",
    "            df = pd.merge(array450k, df, how = \"inner\", left_index=True, right_index=True)\n",
    "            ## Add a stop and reorder the columns\n",
    "            df[\"MAPINFO_Stop\"] = df[\"MAPINFO\"]\n",
    "            df = df[[\"CHR\", \"MAPINFO\", \"MAPINFO_Stop\", \"Beta_Value\"]]\n",
    "            df.sort_values(by = [\"CHR\", \"MAPINFO\"], inplace=True)\n",
    "            df.to_csv(tmp_folder + \"%s.txt\" % file_name , header=None, index=None, sep='\\t', mode = 'w')\n",
    "\n",
    "            # 2. See function above, groups the cg-sites into clusters and merges it to a list with the prespecified label.\n",
    "            df = generateTrain_Infinium(label = \"wbc\", file_name = file_name)\n",
    "            trainFile_list.append(df)\n",
    "\n",
    "        pool = Pool(cpuCount)\n",
    "        pool.map(import_WBC_train, WBC_infinium_files)\n",
    "\n",
    "        ### Read in NBL files\n",
    "        ## Specify files with clinical parameters (MYCN amplified or non-amplified, each line is a sample name)\n",
    "        MNA = pd.read_table('./classifySamples/train/MNA.txt', sep='\\n', header = None)\n",
    "        MA = pd.read_table('./classifySamples/train/MA.txt', header = None)\n",
    "\n",
    "        def import_NBL_train(x):\n",
    "            file = x\n",
    "            file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            name = file_name.split('.')\n",
    "            name = str(name[4])\n",
    "            name = \"-\".join(name.split(\"-\", 3)[:3])\n",
    "            # Extract position and beta value\n",
    "            df = pd.read_csv(file, sep=\"\\t\",usecols=[0,1,2,3,4], header=1)\n",
    "            df[\"Genomic_Coordinate_Stop\"] = df[\"Genomic_Coordinate\"]\n",
    "            df = df[[\"Chromosome\", \"Genomic_Coordinate\", \"Genomic_Coordinate_Stop\", \"Beta_value\"]]\n",
    "            df.sort_values(by = [\"Chromosome\", \"Genomic_Coordinate\"], inplace=True)\n",
    "            df.to_csv(tmp_folder + \"%s.txt\" % file_name , header=None, index=None, sep='\\t', mode = 'w')\n",
    "\n",
    "            if name in MNA.values:\n",
    "                df = generateTrain_Infinium(label = \"nbl-mna\", file_name = file_name)\n",
    "                trainFile_list.append(df)\n",
    "            elif name in MA.values:\n",
    "                df = generateTrain_Infinium(label = \"nbl-ma\", file_name = file_name)\n",
    "                trainFile_list.append(df)\n",
    "            else:\n",
    "                df = generateTrain_Infinium(label = \"nbl-nos\", file_name = file_name)\n",
    "                trainFile_list.append(df)\n",
    "\n",
    "        pool = Pool(cpuCount)\n",
    "        pool.map(import_NBL_train, NBL_infinium_files)\n",
    "\n",
    "        # Similar to NBL, only the column headers are different.\n",
    "        def import_OS_train(x):\n",
    "            file = x\n",
    "            file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            ## Extract position and beta value\n",
    "            df = pd.read_csv(file, sep=\"\\t\",usecols=[1,2,3,4,5], header=0)\n",
    "            ## Add a stop and reorder the columns\n",
    "            df[\"Position_Stop\"] = df[\"Position\"]\n",
    "            df = df[[\"Chromosome\", \"Position\", \"Position_Stop\", \"Signal\"]]\n",
    "            df.sort_values(by = [\"Chromosome\", \"Position\"], inplace=True)\n",
    "            df.to_csv(tmp_folder + \"%s.txt\" % file_name , header=None, index=None, sep='\\t', mode = 'w')\n",
    "\n",
    "            df = generateTrain_Infinium(label = \"os\", file_name = file_name)\n",
    "            trainFile_list.append(df)\n",
    "\n",
    "        pool = Pool(cpuCount)\n",
    "        pool.map(import_OS_train, OS_infinium_files)\n",
    "\n",
    "        # # Similar to NBL, only the column headers are different.\n",
    "        def import_CCSK_train(x):\n",
    "            file = x\n",
    "            file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            ## Extract position and beta value\n",
    "            df = pd.read_csv(file, sep=\"\\t\",usecols=[1,2,3,4,5], header=0, dtype={\"Position\": str})\n",
    "            df\n",
    "            ## Add a stop and reorder the columns\n",
    "            df[\"Position_Stop\"] = df[\"Position\"]\n",
    "            df = df[[\"Chromosome\", \"Position\", \"Position_Stop\", \"AVG_Beta\"]]\n",
    "            df.sort_values(by = [\"Chromosome\", \"Position\"], inplace=True)\n",
    "            df = df.dropna()\n",
    "            df.to_csv(tmp_folder + \"%s.txt\" % file_name , header=None, index=None, sep='\\t', mode = 'w')\n",
    "\n",
    "            df = generateTrain_Infinium(label = \"ccsk\", file_name = file_name)\n",
    "            trainFile_list.append(df)\n",
    "\n",
    "        pool = Pool(cpuCount)\n",
    "        pool.map(import_CCSK_train, CCSK_infinium_files)\n",
    "\n",
    "        ### Similar to NBL, only the column headers are different.\n",
    "        def import_WT_train(x):\n",
    "            file = x\n",
    "            file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            ## Extract position and beta value\n",
    "            df = pd.read_csv(file, sep=\"\\t\",usecols=[1,2,3,4,5], header=0, dtype={\"Position\": str, \"Chromosome\": str})\n",
    "            ## Add a stop and reorder the columns\n",
    "            df[\"Position_Stop\"] = df[\"Position\"]\n",
    "            df = df[[\"Chromosome\", \"Position\", \"Position_Stop\", \"AVG_Beta\"]]\n",
    "            df.sort_values(by = [\"Chromosome\", \"Position\"], inplace=True)\n",
    "            df = df.dropna()\n",
    "            df.to_csv(tmp_folder + \"%s.txt\" % file_name , header=None, index=None, sep='\\t', mode = 'w')\n",
    "\n",
    "            df = generateTrain_Infinium(label = \"wt\", file_name = file_name)\n",
    "            trainFile_list.append(df)\n",
    "\n",
    "        pool = Pool(cpuCount)\n",
    "        pool.map(import_WT_train, WT_infinium_files)\n",
    "\n",
    "        def import_EWS_train(x):\n",
    "            file = x\n",
    "            file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            ## Open the file\n",
    "            df = pd.read_csv(file, sep=\"\\t\", header=None, index_col=0, names = [\"Beta_Value\"])\n",
    "            ## Add the chromosomal position to the sample\n",
    "            df = pd.merge(array450k, df, how = \"inner\", left_index=True, right_index=True)\n",
    "            ## Add a stop and reorder the columns\n",
    "            df[\"MAPINFO_Stop\"] = df[\"MAPINFO\"]\n",
    "            df = df[[\"CHR\", \"MAPINFO\", \"MAPINFO_Stop\", \"Beta_Value\"]]\n",
    "            df.sort_values(by = [\"CHR\", \"MAPINFO\"], inplace=True)\n",
    "            df.to_csv(tmp_folder + \"%s.txt\" % file_name , header=None, index=None, sep='\\t', mode = 'w')\n",
    "\n",
    "            df = generateTrain_Infinium(label = \"ews\", file_name = file_name)\n",
    "            trainFile_list.append(df)\n",
    "\n",
    "        pool = Pool(cpuCount)\n",
    "        pool.map(import_EWS_train, EWS_infinium_files)\n",
    "\n",
    "        def import_MRT_train(x):\n",
    "            file = x\n",
    "            file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            ## Open the file\n",
    "            df = pd.read_csv(file, sep=\"\\t\", header=None, index_col=0, names = [\"Beta_Value\"])\n",
    "            ## Add the chromosomal position to the sample\n",
    "            df = pd.merge(array850k, df, how = \"inner\", left_index=True, right_index=True)\n",
    "            ## Add a stop and reorder the columns\n",
    "            df[\"MAPINFO_Stop\"] = df[\"MAPINFO\"]\n",
    "            df = df[[\"CHR\", \"MAPINFO\", \"MAPINFO_Stop\", \"Beta_Value\"]]\n",
    "            df.sort_values(by = [\"CHR\", \"MAPINFO\"], inplace=True)\n",
    "            df.to_csv(tmp_folder + \"%s.txt\" % file_name , header=None, index=None, sep='\\t', mode = 'w')\n",
    "\n",
    "            df = generateTrain_Infinium(label = \"mrt\", file_name = file_name)\n",
    "            trainFile_list.append(df)\n",
    "\n",
    "        pool = Pool(cpuCount)\n",
    "        pool.map(import_MRT_train, MRT_infinium_files)\n",
    "\n",
    "        ## Specify files with clinical parameters\n",
    "        aRMS = pd.read_table('./classifySamples/train/aRMS.txt', sep='\\n', header = None)\n",
    "        eRMS = pd.read_table('./classifySamples/train/eRMS.txt', header = None)\n",
    "\n",
    "        def import_RMS_train(x):\n",
    "            file = x\n",
    "            file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            ## Open the file\n",
    "            df = pd.read_csv(file, sep=\"\\t\", header=None, index_col=0, names = [\"Beta_Value\"])\n",
    "            ## Add the chromosomal position to the sample\n",
    "            df = pd.merge(array450k, df, how = \"inner\", left_index=True, right_index=True)\n",
    "            ## Add a stop and reorder the columns\n",
    "            df[\"MAPINFO_Stop\"] = df[\"MAPINFO\"]\n",
    "            df = df[[\"CHR\", \"MAPINFO\", \"MAPINFO_Stop\", \"Beta_Value\"]]\n",
    "            df.sort_values(by = [\"CHR\", \"MAPINFO\"], inplace=True)\n",
    "            df.to_csv(tmp_folder + \"%s.txt\" % file_name , header=None, index=None, sep='\\t', mode = 'w')\n",
    "\n",
    "            if file_name in aRMS.values:\n",
    "                df = generateTrain_Infinium(label = \"arms\", file_name = file_name)\n",
    "                trainFile_list.append(df)\n",
    "            elif file_name in eRMS.values:\n",
    "                df = generateTrain_Infinium(label = \"erms\", file_name = file_name)\n",
    "                trainFile_list.append(df)\n",
    "            else:\n",
    "                df = generateTrain_Infinium(label = \"rms-nos\", file_name = file_name)\n",
    "                trainFile_list.append(df)\n",
    "\n",
    "        pool = Pool(cpuCount)\n",
    "        pool.map(import_RMS_train, RMS_infinium_files)\n",
    "\n",
    "        def import_NRML_train(x):\n",
    "            file = x\n",
    "            file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            df = generateTrain_NGS(inputfile = file, label = \"normal\", file_name = file_name)\n",
    "            trainFile_list.append(df)\n",
    "\n",
    "        pool = Pool(cpuCount)\n",
    "        pool.map(import_NRML_train, NRML_WGBS_files)\n",
    "\n",
    "        # Generate full matrix from list\n",
    "        trainFile = pd.concat(trainFile_list, axis = 1)\n",
    "        # Make sure that the file contains all the clusters\n",
    "        trainFile = pd.merge(clusters, trainFile, how = \"left\", left_index=True, right_index=True)\n",
    "        trainFile = trainFile.transpose().fillna('NA')\n",
    "        trainFile.to_csv(\"./classifySamples/output/%s\" % trainFileName, header=None, sep='\\t', mode = 'w')\n",
    "        trainFile_rmNA = trainFile.select_dtypes(include=['float64'])\n",
    "        print(\"The number of columns in the %sfile is: %i\" %  (trainFileName,trainFile.shape[1]))\n",
    "        print(\"The number of columns in the %sfile after removing all NA values is: %i\" %  (trainFileName,trainFile_rmNA.shape[1]))\n",
    "\n",
    "    if visualisation == 'enabled':\n",
    "        trainFile = trainFile.transpose()\n",
    "        testBeta = testBeta.transpose()\n",
    "        TotalMatrix = trainFile\n",
    "\n",
    "        outputfolder = \"./classifySamples/output/plots\"\n",
    "        TotalMatrix = TotalMatrix.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "        print(\"The number of remaining rows in the clustermap is %d (after removing rows containing NA values)\" % (len(TotalMatrix)))\n",
    "        TotalMatrix.to_csv('%s/TotalMatrix.csv' % outputfolder, sep=',', mode='w')\n",
    "\n",
    "        # Get colors for each tumor in the plot\n",
    "        TotalMatrix_labels = TotalMatrix.columns.unique()\n",
    "        TotalMatrix_pal = sns.cubehelix_palette(TotalMatrix_labels.unique().size,\n",
    "                                            light=.9, dark=.1, reverse=True,\n",
    "                                            start=1, rot=-2)\n",
    "        TotalMatrix_lut = dict(zip(map(str, TotalMatrix_labels.unique()), TotalMatrix_pal))\n",
    "        TotalMatrix_colors = pd.Series(TotalMatrix_lut)\n",
    "\n",
    "\n",
    "        print(\"Generating tSNE\")\n",
    "        TotalMatrix = TotalMatrix.transpose()\n",
    "        ## Make a new column with the name of the indices\n",
    "        TotalMatrix['index1'] = TotalMatrix.index\n",
    "        ## Extract the tumor name from the indices\n",
    "        TotalMatrix['tumor'] = TotalMatrix['index1']\n",
    "        TotalMatrix.drop('index1', axis = 1, inplace = True)\n",
    "\n",
    "        matplotlib.rcParams.update({'font.size': 18})\n",
    "        ### tSNE plots\n",
    "        X_tsne = TotalMatrix.drop(\"tumor\", axis = 1)\n",
    "        y_tsne = TotalMatrix['tumor']\n",
    "        print(\"The number of CpGs in the tSNE-plot is: %i\" % len(TotalMatrix.columns))\n",
    "        from sklearn.manifold import TSNE\n",
    "        tsne = TSNE(n_components=2, verbose=1, perplexity=30, n_iter=2000)\n",
    "        tsne_results = tsne.fit_transform(X_tsne)\n",
    "        tsneDf = pd.DataFrame(data = tsne_results\n",
    "                     , columns = ['t-SNE 1', 't-SNE 2'])\n",
    "        y_tsne = y_tsne.values\n",
    "        final_tSNE_Df = pd.concat([tsneDf, pd.DataFrame(y_tsne, columns=[\"target\"])], axis = 1)\n",
    "        print(\"Generating t-SNE plots...\")\n",
    "        fig = plt.figure(figsize = (8,8))\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        ax.set_xlabel('t-SNE 1', fontsize = 18)\n",
    "        ax.set_ylabel('t-SNE 2', fontsize = 18)\n",
    "        ax.set_title('2 component t-SNE', fontsize = 20)\n",
    "        targets = list(TotalMatrix['tumor'].unique())\n",
    "        colors = TotalMatrix_pal\n",
    "        for target, color in zip(targets,colors):\n",
    "            indicesToKeep = final_tSNE_Df['target'] == target\n",
    "            print('Number of %s on t-SNE plot: %d' % (target,len(final_tSNE_Df.loc[indicesToKeep])))\n",
    "            ax.scatter(final_tSNE_Df.loc[indicesToKeep, 't-SNE 1']\n",
    "                       , final_tSNE_Df.loc[indicesToKeep, 't-SNE 2']\n",
    "                       , c = color #pd.Series({\"wbc\": \"#43b7ba\", \"nbl-ma\": \"#ec672e\", \"nbl-mna\": \"#1b2944\", \"NBL1-cfRRBS\":\"#000000\", \"NBL2-cfRRBS\":\"#000000\", \"NBL1-WGBS\":\"#000000\", \"NBL2-WGBS\":\"#000000\", \"NBL1-SeqCapEpi\":\"#000000\", \"NBL2-SeqCapEpi\":\"#000000\"})\n",
    "                       , s = 30)\n",
    "        ax.legend(targets, fontsize = 16, bbox_to_anchor=(1.04,1), loc=\"upper left\", ncol=1, fancybox=True)\n",
    "        ax.grid(False)\n",
    "        fig.show()\n",
    "        fig.savefig('%s/tSNEplot.png' % outputfolder, dpi = 300, bbox_inches=\"tight\")\n",
    "        fig.savefig('%s/tSNEplot.svg' % outputfolder, dpi = 300, bbox_inches=\"tight\")\n",
    "        print(\"Done with plotting tSNE.\")\n",
    "\n",
    "\n",
    "        import umap\n",
    "        X_umap = TotalMatrix.drop(\"tumor\", axis = 1)\n",
    "        y_umap = TotalMatrix['tumor']\n",
    "\n",
    "        print(\"The number of CpGs in the UMAP-plot is: %i\" % len(TotalMatrix.columns))\n",
    "        umap_results = umap.UMAP().fit_transform(X_umap)\n",
    "\n",
    "        umapDf = pd.DataFrame(data = umap_results\n",
    "                     , columns = ['UMAP 1', 'UMAP 2'])\n",
    "        y_umap = y_umap.values\n",
    "        final_umap_Df = pd.concat([umapDf, pd.DataFrame(y_umap, columns=[\"target\"])], axis = 1)\n",
    "        print(\"Generating t-SNE plots...\")\n",
    "        fig = plt.figure(figsize = (8,8))\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        ax.set_xlabel('UMAP 1', fontsize = 18)\n",
    "        ax.set_ylabel('UMAP 1', fontsize = 18)\n",
    "        ax.set_title('2-component UMAP', fontsize = 20)\n",
    "        targets = list(TotalMatrix['tumor'].unique())\n",
    "        colors = TotalMatrix_pal\n",
    "        for target, color in zip(targets,colors):\n",
    "            indicesToKeep = final_umap_Df['target'] == target\n",
    "            print('Number of %s on UMAP plot: %d' % (target,len(final_umap_Df.loc[indicesToKeep])))\n",
    "            ax.scatter(final_umap_Df.loc[indicesToKeep, 'UMAP 1']\n",
    "                       , final_umap_Df.loc[indicesToKeep, 'UMAP 2']\n",
    "                       , c = color\n",
    "                       , s = 30\n",
    "                       )\n",
    "        #ax.legend(targets, fontsize = 16)\n",
    "        ax.legend(targets, fontsize = 16, bbox_to_anchor=(1.04,1), loc=\"upper left\",\n",
    "              ncol=1, fancybox=True)\n",
    "        ax.grid(False)\n",
    "        fig.show()\n",
    "        fig.savefig('%s/UMAPplot.svg' % outputfolder, dpi = 300, bbox_inches=\"tight\")\n",
    "        fig.savefig('%s/UMAPplot.png' % outputfolder, dpi = 300, bbox_inches=\"tight\")\n",
    "        print(\"Done with plotting UMAP.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of clusters is: 14103\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of clusters is: %i\" % len(clusters))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
